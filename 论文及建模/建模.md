

wordle 游戏分析：

## 摘要

Wordle 是一款简单而有趣的文字游戏，目的是猜出一个五个字母的英文单词。玩家需要在六次尝试内猜出正确的单词，每次猜测后，游戏会给出反馈。在2022年，Wordle 游戏风靡全球，因为它非常简单易懂，又有趣又有挑战性，不需要太多的时间和精力，可以随时随地玩。但是，如何针对游戏的特征和用户的行为给出更好的经营策略，仍是一个问题。为此，我们逐个分析

首先，为了分析游戏的热度和模式，我们结合统计学知识，提出了ARIMAX模型。该模型可以预测未来某天游戏的用户人数和选择hard mode的用户人数，并可以在一定的置信水平下给出一个预测区间。以2023年3月1日为例，预测区间为[22319,23211]。同时，我们可以通过ARIMAX模型剔除掉影响模式选择的时间因素，分析游戏的困难模式与单词特征的关系。

其次，为了分析用户在Wordle游戏中的表现，我们结合数据的正态分布特征，选择了LSTM神经网络模型。该模型通过训练单词特征与percentage of answer attempts的关系，可以对给定的单词预测当天用户的猜测次数分布。我们给出了模型不确定性关联的因素，并用MC dropout计算了模型的不确定性。

然后，为了分析题目单词的难度。我们首先利用MLP实现单词的难度系数预测，并给出了依据难度特征分类的标准，将单词分为简单、中等、困难三个类别。为了分析每个分类关联的单词的属性，我们采用kPCA提取每个类别的主要影响因素。通过计算得出EERIE的难度系数，并将其分类为困难的单词。

最后，我们给出了数据集在总人数、用户游戏策略、单词特征这几个方面上的一些有趣的特征。

在文章的结尾，我们对每个模型的参数敏感性进行了分析。我们的框架显示出了很强的新颖性、准确性、鲁棒性，可以泛化到类似的问题中进行分析。

根据我们的模型结论，我们向纽约时报的编辑写了一封信，展示了我们的分析结果并对未来的游戏营销策略提出建议。比如，针对游戏的热度和模式，纽约时报可以加强对游戏的宣传；结合单词难度的特征，题目可以通过增加单词中重复字母的个数或者选择出现频率不高的单词来增加单词的难度，以提高游戏的挑战性。

关键词：ARIMAX 置信区间 LSTM 正态分布 MLP kPCA  dropout





## 模型假设

1. 假设在推特上分享的用户占游戏总用户的绝大部分，这保证reported results的特征可以作为游戏的用户总群体特征。
1. 假设在推特上分享的分数是用户的真实分数，用户不会提前知道答案。
1. 假设没有外在因素，如大幅度的广告投放和媒体推广，或者相似游戏的竞争对用户人数和特征造成显著影响。
1. 假设wordle的单词完全随机，不会迎合固定节日和时政热点。
1. 假设游戏模式和规则不会改变。

## 符号解释

1. $\gamma$表征单词特征的多维向量
2. $letter\_fre_{j}$:每个字母在单词中出现的频率
3. $initial\_fre_{j}$每个字母在单词首字母中出现的频率
4. $word\_letter_{i}$每个单词组成的字母的常见度
5. $letter\_repeat_{i}$每个单词重复的字母个数
6. $word\_fre_{i}$单词在日常生活中出现的常用程度，取值为1-5
7. $word\_class_{i}$单词的词性,名词为1、动词为2、形容词为3、副词为4、其他词语为5
8. $word\_guesstimes_i$单词猜测的次数
9. $word\_difficult_{i}$单词的难度
10. $word\_times_{k}$用户猜中所需要的次数，取值为1-7
11. $word\_precentage_{k}$每个次数对应的百分比
12. $y_t$每天玩wordle的总人数
13. $z_t$每天玩hardmode的人数
14. $per_t$每天玩hardmode的人数占总人数的百分比

## 预处理和单词特征提取

1. 预处理：
   1. 去除单词长度不为5的单词
   1. 去除总人数异常数据

2. 分析单词特征$\gamma_i=(word\_letter_{i},letter\_repeat_{i},word\_fre_{i},word\_class_{i},word\_guesstimes_{i})$：

   1. 每个单词组成的字母的常见度$word\_letter_{i}$

      1. 计算每个字母在单词中出现的频率**【图】**
      2. 计算每个字母出现在首字母的频率$initial\_fre_{j}$**【图】**

      $word\_letter_{i}$定义为$word\_letter_{i}=\sum^{5}_{1}letter\_fre_{j}+initial\_fre_{j}$

   2. 一个单词内字母重复的次数$letter\_repeat_{i}$如果没有重复记为1；如果有一个字母出现两次或两个字母都出现2次(vivid)记为2（例如goose）;如果有一个字母出现三次（eerie，mummy）记为3

   3. 计算单词的常用程度$word\_fre_{i}$：找到xx词频表，词语出现在前3000记频率为5；6000为4；9000为3；12000为2；12000+为1。将词语频率分为5个等级。**【图】**

   4. 计算不同单词的词性$word\_class_{i}$，分析每个词性的频率，词性具体有名词、动词、形容词、副词、其他词语。名词为1、动词为2、形容词为3、副词为4、其他词语为5**【饼图】**

   5. 单词难度$word\_difficult_{i}$：将用户猜中所需要的次数$word\_times_{k}$记为难度等级1-7，每个次数对应的百分比$word\_precentage_{k}$为权重，定义$word\_difficult_{i}=word\_times_{k}\times word\_precentage_{k}$

## problem1:**基于ARIMAX和相关性分析的时间序列预测模型**

1. 使用ARIMA拓展模型ARIMAX对总人数区间进行预测。

   1. ARIMAX模型是ARIMA模型的拓展，可以通过引入外部变量来提高模型预测的准确性。在ARIMAX模型中，除了自变量（ARIMA模型中的p、d、q）之外，还引入一个外部影响变量，通常称为外生变量。

      

   2. 由于节假日与周末可能引起用户上网频率的增加，在这里引入外部影响变量$X_t$，当$t$日为节假日时$X_t=1$否则$X_t=0$

   3. 预测值可以表示为$Y_t = c + \phi_1 Y_{t-1} + ... + \phi_p Y_{t-p} + \theta_1 \epsilon_{t-1} + ... + \theta_q \epsilon_{t-q} + \beta_1 X_{t-1} + ... + \beta_m X_{t-m} + \epsilon_t$。其中，$Y_t$是时间序列数据，$X_t$是外部因素，$\epsilon_t$是白噪声误差。$\phi$和$\theta$是ARIMA模型中的自回归和移动平均系数，$\beta$是ARIMAX模型中的外部因素系数。

      *也可以写成：$$ y_t = \beta^\intercal \mathbf{x}_t + \sum_{i=1}^p \phi_i y_{t-i} + \sum_{j=1}^q \theta_j \varepsilon_{t-j} + \varepsilon_t $$其中，$y_t$为时间序列数据，$\mathbf{x}_t$为外部变量向量，$\beta$为外部变量的系数向量，$p$和$q$为ARIMA模型的阶数，$\phi_i$和$\theta_j$为模型的系数，$\varepsilon_t$为误差项。*

   4. 计算置信区间和置信度：

      1. 模型解释：

         对于该预测值$y_n'$区间估计，我们首先给出$y_n'$的点估计，即ARIMAX的预测值$y_t$。

         其中，$y_1'$，$y_2'$，...,$y_n'$表示该天有可能的取值。

         由于时间序列样本量较大，利用大数定律可以认为过去的误差分布与$y_n'-y_t$相同，为正态分布。

         当$n\to\infin$时，由中心极限定理可知未来玩wordle游戏的总人数的可能取值$y_n'$服从均值为$y_t$，方差为$\sigma^2=s(\varepsilon_t)$的正态分布。

         即：$$ y_n'$$~$$N(y_t,\sigma^2)$$

         对于服从正态分布的变量$ y_s'$，对于一个确定的置信度$\alpha$，置信区间$[\mu-z_{\alpha/2} \times \frac{\sigma}{\sqrt{n}},\mu+z_{\alpha/2} \times \frac{\sigma}{\sqrt{n}}]$表示有$\alpha$的把握， $y_s'$会落在该区间内。其中，$z_{\alpha/2}$表示标准正态分布的分位数。选择$z_{\alpha/2}$而不是$z_{\alpha}$，是因为这样可以确保置信区间的两个边界对应的面积分别为(1-α)/2，从而保证了置信区间的覆盖概率为α。

      2. 具体步骤：

         1. 用ARIMAX模型对目标变量进行预测，得到预测值 $\hat{y}_{t+h}$。**[图片：总人数的预测图]**
         2. 计算ARIMAX模型的残差序列$ \varepsilon_t$。**[图片：拟合和真实值残差的时序图]**
         3. 估计残差项的方差 $\hat{\sigma}^2$。
         4. 计算置信区间。**[三个置信区间，列个表]**

2. 预测3月1日的数据（给一个区间）：给出当$\alpha=10\%$,$\alpha=5\%$,$\alpha=1\%$时的预测区间

3. 单词的属性是否会影响hardmode百分比：

   由于hard mode只能在游戏的开始前被选择，因此hardmode的人数与当天单词特征和难度没有关系。我们认为用户在某天选择hard mode可能是受到前一天的单词特征的影响。
   
   为了更好的研究选择hard mode和单词特征的关系，我们需要排除以下因素对选择hard mode人数的影响：
   
   * 用户总人数的变化
   * 时间的变化
   
   对于第一个因素，可以通过计算当日选择Hard模式的人的百分比来估计选择hard mode占总人数的比例，表示为$per_t$
   
   $\begin{equation}
   per_t=\frac{z_t}{y_t}\times 100\%
   \end{equation}$
   
   对于第二个因素，可以通过XX中的ARIMAX模型计算选择Hard模式的人的百分比的时间趋势，得到仅由时间趋势预测值$hat{per_t}$
   这样得到的残差序列:
   
   $\begin{equation}
   \epsilon'_t=per_t-\hat{per_t}
   \end{equation}$
   
   排除了以上两个因素的影响。通过绘制$\epsilon_t$与前一天的词的特征之间相关性的热力图，我们可以得出结论：
   
   * 前一天的单词中重复的字母个数过多会导致后一天选择hard mode的人数显著减少
   * 前一天的单词不常见或单词字母频率较低会导致后一天选择hard mode的人数稍微减少
   * 前一天单词的词性几乎不影响后一天用户选择hard mode与否。
   
   得到的热图显示在图?????。
   
   

## problem2:**基于正态分布和神经网络的百分比预测模型**

​	在本节中，我们研究了流行的网络游戏Wordle中的单词特征和用户行为之间的关系。我们的分析是基于Wordle的日常数据，每天至少有20,000个用户。
我们采用了一种混合方法预测了用户解谜次数的概率分布，结合了统计和机器学习技术，具体的，是正态分布模型和LSTM神经网络。并给出了单词EERIE的percentages of (1, 2, 3, 4, 5, 6, X) 
我们还分析了模型的 uncertainties，并用dropout计算了模型的confidence.



对于使用LSTM并结合正态分布来预测单词分布的具体步骤如下:

1. 使用LSTM来训练历史数据中单词特征$word\_letter_{i},letter\_repeat_{i},word\_fre_{i},word\_class_{i}$与单词$(\mu_t,\sigma^2_t)$之间的关系
2. 计算给定单词的单词特征，使用LSTM预测其平均数和方差
3. 采用公式
   \`begin{equation}.
   P(Y = l) = (1 / (\sigma \times \sqrt{2π}) \times exp(-(l - \mu)^2 / (2\sigma^2) )
   \end{equation}
   将预测出的正态分布概率函数进行离散化处理，计算单词第x次猜中的概率
4. 对(1, 2, 3, 4, 5, 6, X)进行归一化处理，使得它们的和为1



1. 百分比分布预测模型
   1. 模型原理：

      由于wordle每天的用户数量都至少在20000人以上，因此由伯努利大数定理（这个应该是对的了）可知，（1、2、3、4、5、6、X）的百分比可以看作是当天用户猜测次数为（1、2、3、4、5、6、X）的概率。通过对每天的

   2. 计算$(\mu_t,\sigma^2_t)$，其中$\mu_t=\sum^{7}_{l=1} P_{l}\times l$，$\sigma^2_t=\sum^{7}_{l=1} P_{l}\times (l-\mu)^2$.其中$l$表示用户尝试几次成功，取值为1-7.$P_l$表示第$l$次成功的人数占总人数的百分比    *（其实这里的$\mu_t$应该就是$word\_difficult_{i}$）*

   3. 计算$P(Y = l) = \frac{1} {\sigma \times \sqrt{2π}} \times exp(-(l - μ)^2 / (2σ^2))$

   4. **假如有超过80%的天数的分布符合正态性假设**，我们认为它服从参数为$(\mu_t,\sigma^2_t)$的正态分布

   5. **采用LSTM神经网络模型**，输入层为序列$y_t$,$z_t$,$word\_letter_{i},letter\_repeat_{i},word\_fre_{i},word\_class_{i}$，输出层为$(\mu_t,\sigma^2_t)$，训练神经网络。

   6. 将预测出的正态分布概率函数进行离散化处理:计算

   7. 对每一天预测出的百分比进行归一化处理

2. 具体例子：对eerie的预测

   1. 用problem1的模型预测出$\hat{y_t}$,$\hat{z_t}$

      

   2. 用上述预测出分布$(\hat{\mu_t},\hat{\sigma^2_t})$

      ![image-20230221002204586](C:\Users\86132\AppData\Roaming\Typora\typora-user-images\image-20230221002204586.png)

3. **模型的不确定性What uncertainties are associated with your model and predictions?**

   1.  神经网络的不确定性：

      

   2.  外部因素：在当天的单词、总人数以及玩hardmode的人数已经确定的情况下，如下**外部**因素可能会直接或间接的影响成功次数的百分数

       1. 游戏是否受到媒体推广：当媒体推广游戏时，更多新手倾向于加入游戏。这可能导致单词被猜出来的可能性更低。
       1. 天气因素：当天气较好时，人们倾向于在室内玩休闲游戏，并有更多的时间慢慢推理游戏。这可能导致当日单词被猜出来的可能性更高。
       1. 时政因素：当社会上发生较大新闻事件时，人们倾向于上推特查看新闻热点，并顺手分享一下今日的wordle成绩。这可能导致当日在推特上分享的总人数变多。

4. 模型分析：

   MC Dropout（Monte Carlo Dropout）是一种估计神经网络的不确定性的办法，即在测试时对模型进行多次预测，并在每次预测时随机丢弃一定比例的神经元，使用所有预测结果的平均值作为最终的预测结果，使用所有预测结果的方差作为预测的不确定性。

   在（第二问的第一小问）的LSTM模型中，我们隐藏层设为6层，计算出的平均值$\mu$和方差$\sigma^2$分别为

   （表）

   模型的方差越小，不确定性越小。在敏感性分析中，我们将分析隐藏层的参数取值不同时模型的不确定性。

   具体而言，我们可以将不确定性理解为：对于一个预测值$\hat{y}$，在置信水平为$1-\alpha$的时候，$\hat{y}$的真实值有$\alpha$的概率落在预测区间

   $\hat{y} \pm z_{\alpha/2} \cdot \sqrt{\sigma^2 \cdot (1 + \frac{1}{n} + \frac{(x - \bar{x})^2}{\sum_{i=1}^n(x_i-\bar{x})^2})}$

   内。

   其中，$\hat{y}$是模型的预测值，$z_{\alpha/2}$是标准正态分布的$\alpha/2$分位数，$\sigma^2$是模型的方差，$n$是训练数据的样本数量，$x_i$是要进行预测的输入数据的特征值，$\bar{x}$是训练数据的输入数据特征值的均值。

   预测区间较窄，则说明模型对于预测结果的不确定性较低，我们对于该模型的预测结果比较有信心；如果预测区间较宽，则说明模型对于预测结果的不确定性较高，我们需要更多地考虑模型的预测不确定性。因此使用所有预测结果的方差作为预测的不确定性是合理的。

   

## problem3:**单词难度分类模型**

1. 难度系数的计算：

   在上述问题的模型建立中，我们认为用户猜测的平均次数$word\_guesstimes_i$可以表示单词的难度$word\_difficulty_i$。但是对于一个没有出现在历史记录里的单词，我们无法根据猜测成功次数计算难度。因此，我们需要依据单词特征预测单词难度，以此为后面的单词难度分类做铺垫。

   本小问中，我们采用the multilayer perceptron (MLP)对单词的难度进行分类。MLP（Muti-Layer Perception）是一种简单的人工神经网络，常用于逻辑回归与非线 性分类问题，由一个输入层、一个输出层以及多个隐藏层组成，每一层由若干个非线性的单元节点（神经元）组成。这是一种前向结构的人工神经网络ANN，可以映射一组输入向量到一组输出向量，克服了感知器不能对线性不可分数据进行识别的弱点。具体步骤如下：

    1. 网络初始化。初始化每一个神经元的权重和偏置值，通常采用随机初始化。权重和偏置的初始化公式为：

       $W^{(0)}_{i,j} \sim N(0, \sigma^2) \\
       b^{(0)}_i = 0$

       其中，$W_{i,j}$是连接第$j$个神经元和第$i$个神经元之间的权重，$\mathcal{N}(0, \sigma^2)$表示服从均值为0，方差为$\sigma^2$的正态分布，$b_i$是第$i$个神经元的偏置值。

    2. 前向传播。输入数据从输入层开始，通过多个隐层，最终到达输出层。在每个神经元中，需要计算输入信号的加权和，然后通过激活函数进行非线性转换，得到输出值。输出值作为下一层神经元的输入值。激活函数的作用是将非线性引入神经元的输出。对于每个隐藏层的神经元，通过加权和和激活函数计算输出。对于输出层的神经元，也通过加权和和激活函数计算输出。其中，第$i$层的第$j$个神经元的输出$y_{i,j}$的计算公式为：

       $$z_i = \sum_{j=1}^{n} W_{i,j}x_j + b_i \\
       h_i = f(z_i)$$

       其中，$W_{k,j}$表示连接第$k$个神经元和第$j$个神经元之间的权重，$y_{i-1,k}$表示第$i-1$层的第$k$个神经元的输出，$b_j$是第$j$个神经元的偏置值，$f$是激活函数。通常情况下，激活函数有：

       | 激活函数 | 函数公式                                  |
       | -------- | ----------------------------------------- |
       | Sigmoid  | $f(x) = \frac{1}{1 + e^{-x}}$             |
       | Identity | $f(x)=x$                                  |
       | ReLU     | $f(x)=max(0, x)$                          |
       | Tanh     | $f(x) = \frac{e^x- e^{-x}}{e^x + e^{-x}}$ |

       针对本题的数据，经过多次试验，我们创建新的激活函数Tanh-ReLU以结合它们在不同难度的单词的预测效果。激活函数Tanh-ReLU的数学公式定义如下：

       $$f(x) = \begin{cases}  
       x & x \ge 0 \\\
       \frac{e^x- e^{-x}}{e^x + e^{-x}} & x \textless 0 
       \end{cases}$$

       在后续的敏感性分析中，我们会分析这几个激活函数的优劣，并说明我们创建这个激活函数的原因。

    3. 使用交叉熵计算损失函数，衡量模型的预测结果和真实结果之间的差距。常用的损失函数包括均方误差（MSE）、交叉熵等。以均方误差为例，它的计算公式为：

       $$L(y, \hat{y}) = \frac{1}{N}\sum_{i=1}^{N} l(y_i, \hat{y}_i)$$

       其中，$n$是样本数量，$y_i$是实际输出，$\hat{y}_i$是目标输出。

    4. 反向传播。在反向传播过程中，先计算输出层误差的梯度，然后逐层向前计算隐层误差的梯度。最后，根据梯度更新模型的权重和偏置，公式为：

   $$W_{i,j}^{(t+1)} = W_{i,j}^{(t)} - \eta \frac{\partial L}{\partial W_{i,j}} \\
   b_i^{(t+1)} = b_i^{(t)} - \eta \frac{\partial L}{\partial b_i}$$

   其中，$h_i$是第$i$个神经元的输出值，$z_i$是第$i$个神经元的输入值，$l(y_i, \hat{y}_i)$是损失函数中的第$i$个样本的损失，$\delta_i$是第$i$个神经元的误差项，$\frac{\partial L}{\partial h_i}$是损失函数对$h_i$的偏导数，$\frac{\partial h_i}{\partial z_i}$是激活函数对$z_i$的偏导数。

   5. 迭代训练。将训练集随机分成若干个小批次，对每个小批次进行前向传播、计算损失函数、反向传播、参数更新等操作。最终得到了单词难度系数判断模型。

2. 难度单词的分类：

   为了更好的将单词按难度系数评级，我们通过观察给定数据的$word\_guesstimes_i$范围，对单词的难度水平等分三个级别，分别是：

   | 难度 | $word\_guesstimes_i$区间范围 |
   | ---- | ---------------------------- |
   | 简单 | [2,3.5]                      |
   | 中等 | [3.5,4]                      |
   | 困难 | [4,6]                        |

   ​	这样，对于给定的单词，我们可以计算它的单词特征$\gamma_i=(word\_letter_{i},letter\_repeat_{i},word\_fre_{i},word\_class_{i})$通过上述神经网络给出预测的难度系数$\widehat{word\_difficulty_i}$，并将单词分成简单，中等或者困难的类别中。

   ​	比如，对于EERIE这个单词，我们可以计算特征（）得到它应该分到（）类中。

   

3. 与难度分类关联的单词属性

   ​	为了得到每个难度类别关联的单词属性，我们将每一个难度的单词特征进行核主成分分析（kPCA）。标准的PCA是一种线性变换，只能处理线性相关的数据。但是在本问中，单词特征可能具有非线性关系，这时候就需要使用非线性PCA，即核主成分分析。在kPCA中，我们首先将原始数据映射到一个高维的特征空间，然后在该特征空间中执行PCA，获得特征矩阵的特征值和特征向量。

   ​	本题中kPCA（Kernel PCA）的具体步骤如下：

   1. 对于$m=355$个样本$x_1, x_2, \ldots, x_m$，其中每个样本有$n=4$个特征。定义核函数$k(x, y)$，用于将原始数据映射到一个高维的特征空间中。

   2. 计算核矩阵$K$，其中$K_{ij} = k(x_i, x_j)$。该核矩阵包含了原始数据在高维空间中的相似度信息。

   3. 对核矩阵$K$进行中心化，即$K \leftarrow H K H$，其中$H = I - \frac{1}{m} 11^T$，$I$为单位矩阵，$1$是全1向量。这一步是为了消除核矩阵的平移和缩放。

   4. 对中心化后的核矩阵$K$进行特征值分解，得到特征向量$v_1, v_2, \ldots, v_m$和相应的特征值$\lambda_1, \lambda_2, \ldots, \lambda_m$。这些特征向量构成了数据在高维特征空间中的主成分，对应的特征值表示了每个主成分的重要程度。

   5. 选择前$k=3$个特征向量，构成一个新的变换矩阵$V_k = [v_1, v_2, \ldots, v_k]$，将原始数据$x_i$映射到$k$维空间中：$\phi(x_i) = [v_1^T x_i, v_2^T x_i, \ldots, v_k^T x_i]$。

      ​	映射后的数据是由$k$个特征值组成的向量，每个特征值对应着一个特征向量，描述了数据在该方向上的主要变化方向和大小。特征值和特征向量如下：（图or表）

      对于简单难度的单词，主成分分析如下：

      | 序号 | 特征值 | 贡献率 |
      | ---- | ------ | ------ |
      | 1    |        |        |
      | 2    |        |        |
      | 3    |        |        |
      
      标准化变量的前两个主成分对应的特征向量为：
      
      |      | y_1  | y_2  | y_3  |
      | ---- | ---- | ---- | ---- |
      |      |      |      |      |
      |      |      |      |      |
      
      中等难度的单词、、、、困难难度的单词、、、、
      
      
      
      
      
      
      
      通过观察不同难度的单词组特征向量系数的不同，我们总结三个难度关联的单词属性如下：
      
      （大概是一个表）
      
      | 关联         | 简单           | 中等           | 困难           |
      | ------------ | -------------- | -------------- | -------------- |
      | 首要关联特征 | $word\_letter$ | $word\_fre$    | $word\_repeat$ |
      | 次要关联特征 | $word\_repeat$ | $word\_letter$ | $word\_letter$ |
      
      
      
      ​	比如，对于EERIE这个单词，我们可以看到它（）特征特别显著，和（）分类相关。与第二小问的结果相吻合，可知我们的模型有一定的可靠性。

4. 模型准确度评估

   对于WLP的模型误差，我们用交叉验证来评估神经网络性能。在交叉验证过程中，数据集被分成若干个不重叠的子集，其中一个子集被保留用于测试，其他子集用于训练神经网络。这个过程会被重复多次，每次都会用不同的子集作为测试集，从而得到多个性能指标的估计值。这些估计值可以用来计算神经网络的平均性能指标。交叉验证可以减小模型过拟合的风险，提高模型的泛化能力。通过多次使用不同的测试集，可以更准确地估计神经网络的性能，避免因为某个测试集的选择导致评估结果的偏差。

    在 k 折交叉验证中，具体而言，我们采用5折交叉验证。具体步骤如下：

   1. 选取80%的数据用于训练神经网络。

   2. 用训练出的神经网络预测剩余20%的数据。

   3. 计算预测值的MSE（Mean Squared Error）和RMSE（Root Mean Squared Error）

      $MSE=\frac{1}{n}*\sum(\widehat{y}-y)^2$

      $RMSE=\sqrt{\frac{1}{n}}*\sum(\widehat{y}-y)^2$

   4. 更换数据集，重复计算MSE和RMSE

   我们将不同数据集计算出的MSE和RMSE的数据表示为下图：

   （图）

   可以看到MSE和RMSE均在（blabla）范围内，可知模型准确度较好。

   

## problem4:有趣的特征

1. 有关单词特征

   1. 词语频率：对word_fre做平滑处理画图
   2. 单词在各个位置出现的频率
2. 有关用户人数

   1. wordle的用户人数在2022年1、2月份迅速上升，到达峰值。可能因为是在1、2月份纽约时报买下了wordle并进行了大力宣传。
   1. 用户人数在2月份之后迅速下降，可能是因为在纽约时报收购了之后，添加了广告或者进行了收费，导致原先用户大量流失。

3. 有关用户游戏策略：对一次即成功的百分比大于1%的单词，分析它的$(word\_letter_{i},letter\_repeat_{i},word\_fre_{i})$

   2. 对大于6次仍未成功的百分比大于10%的单词，分析$(word\_letter_{i},letter\_repeat_{i},word\_fre_{i})$
      1. 举例分析48%
      1. 选择游戏模式：选择hardmode的百分比逐渐上涨，人们可能由于对游戏越来越熟悉而选择hardmode挑战自己
      1. 猜测的平均次数：对word_difficult做平滑处理

## 敏感性分析：

1. ARIMAX的参数敏感性分析：改变不同的参数，误差的大小【图】和改变不同的参数，预测结果的变化【图】

2. LSTM神经网络的层数敏感性分析：不同层数敏感性的不确定度（dropout）

3. MLP激活函数敏感性分析
   
   MLP中激活函数的引入使得神经元的输出可以被限制在一定的范围内，这种限制可以让神经网络更好地适应输入数据的分布，避免过拟合和欠拟合的情况。不同的激活函数有不同的特点，根据不同的任务和网络结构，选择合适的激活函数可以提高神经网络的性能。
   
   | 激活函数  | 函数公式                                                     |
   | --------- | ------------------------------------------------------------ |
   | Sigmoid   | f(x) = \frac{1}{1 + e^{-x}}                                  |
   | Identity  | f(x)=x                                                       |
   | ReLU      | f(x)=max(0, x)                                               |
   | Tanh      | f(x) = \frac{e^x- e^{-x}}{e^x + e^{-x}}                      |
   | Tanh-ReLU | $$f(x) = \begin{cases}  x & x \ge 0 \\\frac{e^x- e^{-x}}{e^x + e^{-x}} & x \textless 0 \end{cases}$$ |
   
   对于四个备选激活函数和本模型自创的激活函数，我们分别计算它在不同难度的单词上预测的MSE和RMSE，以比较激活函数在不同难度的单词的预测中性能的好坏。
   
   对于难度小于平均值的单词，五个函数的误差分别是：
   
   （图）
   
   激活函数tanh误差最小，效果最好。logistic，identity其次，ReLU最差。
   
   对于难度大于平均值的单词，五个函数的误差分别是：
   
   （图）
   
   激活函数ReLU误差最小，效果最好。logistic，identity其次，tanh最差。
   
   我们受到启发，创建新的激活函数Tanh-ReLU：$$f(x) = \begin{cases}  x & x \ge 0 \\\frac{e^x- e^{-x}}{e^x + e^{-x}} & x \textless 0 \end{cases}$$，以结合Tanh在低难度单词和ReLU在高难度单词的优势。
   
   对于所有单词，五个函数的误差分别是：
   
   激活函数Tanh-ReLU误差最小，效果最好。logistic，identity其次，tanh最差。
   



对于不同难度的单词的误差，五个函数表现出了不同的优势。对于简单的单词，tanh误差最小，效果最好。对于困难的单词，ReLU误差最小，效果最好。对于所有的单词，Tanh-ReLU误差最小，效果最好。

## 写信：

1. （人数）自2022年2月开始，

2. 模式

3. 做法：为了吸引更多的用户，给用户带来更好的游戏体验，游戏

   

## 模型评价

1. 优点
   1. 对方法的合理使用。本模型在使用常见方法的时候，都根据问题的特性对方法进行了合理的调整，使方法更适合于具体问题。比如：
   
       \begin{itemize}
              \item We extended ARIMA to ARIMAX model by considering other variables that may affect ARIMA.
              \item We derived the mathematical distribution of reported score percentage and used LSTM to predict the distribution of possible successful attempts by first predicting the mean and variance of user's guessing attempts.
              \item We analyzed the effect of different activation functions in WLP on predicting words of different difficulties and introduced the Tanh-ReLU activation function to reduce errors.
              \item We considered the non-linearity of word features and used non-linear PCA, namely kernel principal component analysis.
          \end{itemize}
   
      1. 在使用ARIMA时，我们考虑了可能影响的其他变量，将ARIMA扩展为ARIMAX模型。
      2. 在使用LSTM时，我们推导了报告的分数百分比的数学分布，通过预测用户猜测次数的平均值和方差来预测用户可能的成功次数分布。
      3. 在使用WLP时，我们分析了不同激活函数对不同难度单词预测的效果，自创了Tanh-ReLU激活函数以减小误差。
      4. 在使用PCA时，我们考虑到单词特征可能是非线性的，使用了非线性PCA，即核主成分分析。
   
   2. 多样的评估方法。针对不同的模型，我们采用了不同的评估方法来评价模型的有效性。比如，采用了dropout层对LSTM的不确定性进行分析，采用了交叉分析对WLP的准确度进行分析。
   
   3. 详细的敏感性分析。本模型对ARIMAX的参数，LSTM神经网络的层数，MLP的激活函数都进行了详细的敏感性分析。显示了（？）
   
   4. 与数学原理结合。本模型结合了统计学中大数定律，相关关系，参数的点估计与区间估计等相关内容，对置信区间和模型确定性进行了推导和说明。
   
2. 缺点
   1. 单词特征的提取不足。本模型并未将单词字母属于元音或者辅音作为特征进行提取。也并没有考虑字母一些常见的组合对猜测难度的影响。
   2. 主成分分析的贡献值不够。由于单词的特征没有明显区别，所以使用kPCA分解出的主成分贡献率不大。
   3. 由于数据集较小，神经网络可能出现过拟合问题。

## 模型结论

为了更好地设计wordle并给用户带来更好的游戏体验，我们提出了一系列新颖的模型来预测wordle的用户人数、选择hard mode的用户人数，并分析了单词特征与难度的相关关系，

1. 我们构建了考虑外部变量的时间序列预测模型ARIMAX。这有利于分析随着时间增长用户人数和选择hardmode人数的变化，也有利于分析
2. 我们构建了基于正态分布和LSTM的百分比预测模型，对于给定的单词，我们可以通过
3. 我们构建了基于MLP的难度预测模型，依据单词的难度系数进行分类，并通过kPCA分析了难度和单词特征的关系。
4. 最后，列出了一些给定数据中有趣的特征，并给纽约时报的编辑写了一份信提供有关游戏的建议和策略。